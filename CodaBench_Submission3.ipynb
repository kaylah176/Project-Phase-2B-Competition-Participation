{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Car Review Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Target\n",
      "0  Bought 2017 Optima Hybrid in November 17. \\nIt...       2\n",
      "1  You get a lot for your money and great perform...       2\n",
      "2  This car is amazing and have no complaints. Yo...       2\n",
      "3  At 11k now in a lease for 39 months and it onl...       0\n",
      "4  I've owned BMW, Lexus, Mercedes-Benz in the la...       2\n",
      "0    Bought 2017 Optima Hybrid in November 17. \\nIt...\n",
      "1    You get a lot for your money and great perform...\n",
      "2    This car is amazing and have no complaints. Yo...\n",
      "3    At 11k now in a lease for 39 months and it onl...\n",
      "4    I've owned BMW, Lexus, Mercedes-Benz in the la...\n",
      "Name: Review, dtype: object\n",
      "0    2\n",
      "1    2\n",
      "2    2\n",
      "3    0\n",
      "4    2\n",
      "Name: Target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file\n",
    "train_df = pd.read_excel('/Users/kaylahoffman/Downloads/Train_data.xlsx')\n",
    "\n",
    "# Display the first few rows to verify the data\n",
    "print(train_df.head())\n",
    "\n",
    "# Split the data into X_train and y_train\n",
    "X = train_df['Review']\n",
    "y = train_df['Target']\n",
    "\n",
    "# Display the first few entries of X_train and y_train\n",
    "print(X.head())\n",
    "print(y.head())\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and install necessary libaries \n",
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# !pip install transformers\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizier\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input texts\n",
    "X_train_tokenized = tokenizer(X_train.tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "X_val_tokenized = tokenizer(X_val.tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(y.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the training parameters \n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_tokenized['input_ids'], X_train_tokenized['attention_mask'], torch.tensor(y_train.tolist()))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 completed\n",
      "Epoch 2/3 completed\n",
      "Epoch 3/3 completed\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = TensorDataset(X_val_tokenized['input_ids'], X_val_tokenized['attention_mask'], torch.tensor(y_val.tolist()))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7073170731707317\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.00      0.00      0.00         4\n",
      "           2       0.71      1.00      0.83        29\n",
      "\n",
      "    accuracy                           0.71        41\n",
      "   macro avg       0.24      0.33      0.28        41\n",
      "weighted avg       0.50      0.71      0.59        41\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "actual_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        predictions.extend(preds.tolist())\n",
    "        actual_labels.extend(labels.tolist())\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print(f\"Accuracy: {accuracy_score(actual_labels, predictions)}\")\n",
    "print(classification_report(actual_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'learning_rate': 1e-5, 'batch_size': 16, 'num_epochs': 3, 'dropout': 0.1},\n",
    "    {'learning_rate': 2e-5, 'batch_size': 32, 'num_epochs': 4, 'dropout': 0.2},\n",
    "    # Add more parameter combinations as needed\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6829268292682927\n"
     ]
    }
   ],
   "source": [
    "# Function to train and evaluate model\n",
    "def train_evaluate_model(params):\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(set(y)))\n",
    "    model.dropout = torch.nn.Dropout(params['dropout'])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=params['batch_size'])\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=params['learning_rate'])\n",
    "    \n",
    "    # Learning rate scheduler with warmup\n",
    "    total_steps = len(train_loader) * params['num_epochs']\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    for epoch in range(params['num_epochs']):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "# Evaluation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss += outputs.loss.item()\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / len(val_dataset)\n",
    "    return accuracy\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 1e-05, 'batch_size': 16, 'num_epochs': 3, 'dropout': 0.1}\n",
      "Best validation accuracy: 0.7073170731707317\n"
     ]
    }
   ],
   "source": [
    "# Grid search\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "\n",
    "for params in param_grid:\n",
    "    accuracy = train_evaluate_model(params)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best validation accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the test dataset\n",
    "test_df = pd.read_excel('/Users/kaylahoffman/Downloads/test_features.xlsx')\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and encode the test data\n",
    "encoded_test = tokenizer(test_df['Review'].tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "# Create a TensorDataset for the test data\n",
    "test_dataset = torch.utils.data.TensorDataset(encoded_test['input_ids'], encoded_test['attention_mask'])\n",
    "\n",
    "# Create a DataLoader for batch processing\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(y.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask = batch\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Convert predictions to labels if needed\n",
    "# predicted_labels = [label_map[pred] for pred in all_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions have been added to the test dataset and saved to 'test_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "# Add predictions to the test dataframe\n",
    "test_df['Predicted_Target'] = all_predictions\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "test_df.to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "print(\"Predictions have been added to the test dataset and saved to 'test_predictions.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kaylahoffman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/kaylahoffman/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3800'>3801</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3801'>3802</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3802'>3803</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Target'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_df\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Apply augmentation to your training data\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m augmented_train_df \u001b[38;5;241m=\u001b[39m \u001b[43maugment_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Shuffle the final dataset\u001b[39;00m\n\u001b[1;32m     88\u001b[0m augmented_train_df \u001b[38;5;241m=\u001b[39m augmented_train_df\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[72], line 68\u001b[0m, in \u001b[0;36maugment_dataset\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Augment class 0 and 1 samples\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]:  \u001b[38;5;66;03m# Minority classes\u001b[39;00m\n\u001b[1;32m     69\u001b[0m         augmented_texts \u001b[38;5;241m=\u001b[39m augment_data(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m aug_text \u001b[38;5;129;01min\u001b[39;00m augmented_texts:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/pandas/core/series.py?line=977'>978</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m    <a href='file:///Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/pandas/core/series.py?line=979'>980</a>\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> <a href='file:///Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/pandas/core/series.py?line=980'>981</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m    <a href='file:///Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/pandas/core/series.py?line=982'>983</a>\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    <a href='file:///Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/pandas/core/series.py?line=983'>984</a>\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/pandas/core/series.py?line=984'>985</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/pandas/core/series.py?line=985'>986</a>\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/pandas/core/series.py?line=1085'>1086</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   <a href='file:///Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/pandas/core/series.py?line=1087'>1088</a>\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/pandas/core/series.py?line=1088'>1089</a>\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   <a href='file:///Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/pandas/core/series.py?line=1089'>1090</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3801'>3802</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3802'>3803</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> <a href='file:///Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3803'>3804</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3804'>3805</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3805'>3806</a>\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3806'>3807</a>\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3807'>3808</a>\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/kaylahoffman/miniconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3808'>3809</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Target'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def augment_data(text, num_augmentations=2):\n",
    "    augmented_texts = []\n",
    "    \n",
    "    # 1. Synonym Replacement\n",
    "    def synonym_replacement(sentence):\n",
    "        words = sentence.split()\n",
    "        new_words = words.copy()\n",
    "        random_word_list = list(set([word for word in words if wordnet.synsets(word)]))\n",
    "        random.shuffle(random_word_list)\n",
    "        num_replaced = 0\n",
    "        for random_word in random_word_list:\n",
    "            synonyms = wordnet.synsets(random_word)\n",
    "            if len(synonyms) >= 1:\n",
    "                synonym = random.choice(synonyms).lemmas()[0].name()\n",
    "                new_words = [synonym if word == random_word else word for word in new_words]\n",
    "                num_replaced += 1\n",
    "            if num_replaced >= 2:  # Replace up to 2 words\n",
    "                break\n",
    "        return ' '.join(new_words)\n",
    "    \n",
    "    # 2. Random Deletion\n",
    "    def random_deletion(sentence, p=0.1):\n",
    "        words = sentence.split()\n",
    "        if len(words) == 1:\n",
    "            return sentence\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if random.uniform(0, 1) > p:\n",
    "                new_words.append(word)\n",
    "        if len(new_words) == 0:\n",
    "            rand_int = random.randint(0, len(words)-1)\n",
    "            new_words.append(words[rand_int])\n",
    "        return ' '.join(new_words)\n",
    "    \n",
    "    # 3. Random Swap\n",
    "    def random_swap(sentence, n=2):\n",
    "        words = sentence.split()\n",
    "        if len(words) < 2:\n",
    "            return sentence\n",
    "        for _ in range(n):\n",
    "            idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "            words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    # Apply augmentations\n",
    "    augmented_texts.append(synonym_replacement(text))\n",
    "    augmented_texts.append(random_deletion(text))\n",
    "    augmented_texts.append(random_swap(text))\n",
    "    \n",
    "    return augmented_texts[:num_augmentations]\n",
    "\n",
    "# Apply augmentation to the minority classes (0 and 1)\n",
    "def augment_dataset(df):\n",
    "    augmented_data = []\n",
    "    \n",
    "    # Augment class 0 and 1 samples\n",
    "    for idx, row in df.iterrows():\n",
    "        if row['Target'] in [0, 1]:  # Minority classes\n",
    "            augmented_texts = augment_data(row['Review'])\n",
    "            for aug_text in augmented_texts:\n",
    "                augmented_data.append({\n",
    "                    'Review': aug_text,\n",
    "                    'Target': row['Target']\n",
    "                })\n",
    "    \n",
    "    # Create DataFrame with augmented data\n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "    \n",
    "    # Combine original and augmented data\n",
    "    final_df = pd.concat([df, augmented_df], ignore_index=True)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Apply augmentation to your training data\n",
    "augmented_train_df = augment_dataset(train_df)\n",
    "\n",
    "# Shuffle the final dataset\n",
    "augmented_train_df = augmented_train_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(f\"Original dataset size: {len(train_df)}\")\n",
    "print(f\"Augmented dataset size: {len(augmented_train_df)}\")\n",
    "print(\"\\nClass distribution in augmented dataset:\")\n",
    "print(augmented_train_df['Target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
